{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/personal_logo.png\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Unsupervised Learning - Final Project\n",
    "### Juan Carlos Garzon Pico\n",
    "### Viviane Alves\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "  \n",
    "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/Juank0621)\n",
    "[![LinkedIn](https://img.shields.io/badge/LinkedIn-Profile-blue?logo=linkedin)](https://www.linkedin.com/in/juancarlosgarzon)\n",
    "![Python](https://badges.aleen42.com/src/python.svg)\n",
    "\n",
    "</div>\n",
    "\n",
    "### CIFAR10 AI System\n",
    "\n",
    "We are developing an AI system using deep learning techniques like Convolutional Autoencoders (CAE), Variational Autoencoders (VAE), and Generative Adversarial Networks (GANs) with the CIFAR10 dataset. These models will help in facial feature extraction, attribute classification, and image generation. By leveraging these approaches, we aim to enhance face recognition, noise reduction, and synthetic face generation for improved image analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm   \n",
    "#from tqdm.rich import tqdm  # Import tqdm.rich for progress bars\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18  # Import ResNet18\n",
    "from torchsummary import summary\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch and GPU Information\n",
    "\n",
    "This code snippet displays the PyTorch version, CUDA version, cuDNN version, and the number of GPUs available for PyTorch.\n",
    "\n",
    "The first line prints the PyTorch version being used.\n",
    "The second and third lines retrieve and display the CUDA and cuDNN versions used by PyTorch.\n",
    "The final line shows the number of GPUs available for PyTorch, helping to confirm whether your system is utilizing the GPU for processing.\n",
    "This is useful for ensuring that your environment is correctly set up to use GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch Version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Version: 12.4\n",
      "cuDNN Version: 90100\n"
     ]
    }
   ],
   "source": [
    "# Get the CUDA version used by PyTorch\n",
    "cuda_version = torch.version.cuda\n",
    "print(\"CUDA Version:\", cuda_version)\n",
    "\n",
    "# Get the cuDNN version used by PyTorch\n",
    "cudnn_version = torch.backends.cudnn.version()\n",
    "print(\"cuDNN Version:\", cudnn_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 1\n"
     ]
    }
   ],
   "source": [
    "# Get the number of GPUs available\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Num GPUs Available:\", num_gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr  4 11:07:29 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4080        Off | 00000000:01:00.0  On |                  N/A |\n",
      "| 30%   32C    P2              35W / 320W |    840MiB / 16376MiB |     10%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      4425      G   /usr/lib/xorg/Xorg                          159MiB |\n",
      "|    0   N/A  N/A      5149      G   /usr/bin/gnome-shell                         88MiB |\n",
      "|    0   N/A  N/A      8293      G   ...erProcess --variations-seed-version      241MiB |\n",
      "|    0   N/A  N/A      8435      G   ...17eb9633ccae6fd3ecfd91c4bbeee74e2c3       37MiB |\n",
      "|    0   N/A  N/A    109052      C   /usr/share/rustdesk/rustdesk                274MiB |\n",
      "|    0   N/A  N/A    144504      G   /usr/share/rustdesk/rustdesk                 15MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the transformations to be applied to the images in the CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 8, 'pin_memory': True} # DataLoader optimization for better performance on CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Network (GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CelebA Dataset for Image Super-Resolution\n",
    "\n",
    "We will use the CelebA dataset for training a Variational Autoencoder (VAE) combined with a Generative Adversarial Network (GAN) to perform image super-resolution.\n",
    "\n",
    "### Workflow\n",
    "\n",
    "1. **Dataset Preparation**: Load the CelebA dataset and preprocess it for low-resolution and high-resolution image pairs.\n",
    "2. **Model Architecture**: Combine VAE and GAN to create a VAE-GAN model for super-resolution.\n",
    "3. **Training**: Train the model to generate high-resolution images from low-resolution inputs.\n",
    "4. **Evaluation**: Use metrics like PSNR and SSIM to evaluate the quality of the generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CelebA dataset\n",
    "from torchvision.datasets import CelebA\n",
    "\n",
    "# Define transformations for CelebA dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(178),  # Crop to square\n",
    "    transforms.Resize((64, 64)),  # High-resolution target\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CelebA dataset\n",
    "celeba_dataset = CelebA(root='./data', split='train', transform=transform, download=True)\n",
    "\n",
    "# Create low-resolution version for input\n",
    "low_res_transform = transforms.Compose([\n",
    "    transforms.Resize((16, 16)),  # Low-resolution input\n",
    "    transforms.Resize((64, 64)),  # Upscale back to match high-resolution size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "celeba_low_res_dataset = CelebA(root='./data', split='train', transform=low_res_transform, download=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE-GAN Architecture\n",
    "\n",
    "The VAE-GAN combines the Variational Autoencoder (VAE) with a Generative Adversarial Network (GAN) to improve the quality of generated images. The VAE acts as the generator, and the GAN discriminator ensures the generated images are realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VAE-GAN Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 4 * 4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VAE-GAN Training Loop\n",
    "def train_vae_gan(vae, discriminator, dataloader, optimizer_vae, optimizer_disc, num_epochs=50):\n",
    "    criterion = nn.BCELoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        for high_res, low_res in zip(celeba_dataset, celeba_low_res_dataset):\n",
    "            high_res = high_res[0].unsqueeze(0).to(device)\n",
    "            low_res = low_res[0].unsqueeze(0).to(device)\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_disc.zero_grad()\n",
    "            real_labels = torch.ones((high_res.size(0), 1)).to(device)\n",
    "            fake_labels = torch.zeros((high_res.size(0), 1)).to(device)\n",
    "\n",
    "            real_output = discriminator(high_res)\n",
    "            fake_images, _, _ = vae(low_res)\n",
    "            fake_output = discriminator(fake_images.detach())\n",
    "\n",
    "            real_loss = criterion(real_output, real_labels)\n",
    "            fake_loss = criterion(fake_output, fake_labels)\n",
    "            disc_loss = real_loss + fake_loss\n",
    "            disc_loss.backward()\n",
    "            optimizer_disc.step()\n",
    "\n",
    "            # Train VAE (Generator)\n",
    "            optimizer_vae.zero_grad()\n",
    "            fake_output = discriminator(fake_images)\n",
    "            gen_loss = criterion(fake_output, real_labels)\n",
    "            vae_loss = vae_loss_function(low_res, fake_images, _, _) + gen_loss\n",
    "            vae_loss.backward()\n",
    "            optimizer_vae.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Disc Loss: {disc_loss.item():.4f}, VAE Loss: {vae_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics: PSNR and SSIM\n",
    "\n",
    "We use Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM) to evaluate the quality of the generated high-resolution images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def evaluate_metrics(high_res, generated):\n",
    "    high_res_np = high_res.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "    generated_np = generated.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "    psnr_value = psnr(high_res_np, generated_np, data_range=1.0)\n",
    "    ssim_value = ssim(high_res_np, generated_np, multichannel=True, data_range=1.0)\n",
    "\n",
    "    return psnr_value, ssim_value\n",
    "\n",
    "# Example usage\n",
    "high_res, low_res = celeba_dataset[0][0].unsqueeze(0).to(device), celeba_low_res_dataset[0][0].unsqueeze(0).to(device)\n",
    "generated, _, _ = vae(low_res)\n",
    "psnr_value, ssim_value = evaluate_metrics(high_res, generated)\n",
    "print(f\"PSNR: {psnr_value:.2f}, SSIM: {ssim_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "\n",
    "```bibtex\n",
    "@inproceedings{liu2015faceattributes,\n",
    "  title = {Deep Learning Face Attributes in the Wild},\n",
    "  author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},\n",
    "  booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},\n",
    "  month = {December},\n",
    "  year = {2015} \n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dul",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
